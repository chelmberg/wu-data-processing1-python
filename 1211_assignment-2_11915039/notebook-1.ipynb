{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3af23f1c064104477deb252c4788d80e",
     "grade": false,
     "grade_id": "cell-e37bf3469fc1e9d7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). Do NOT add any cells to the notebook!\n",
    "\n",
    "Do not forget to submit both the notebook AND the files in the data/ subfolder according to the CoC!\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or _YOUR ANSWER HERE_ , as well as your name and group below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Christoph Helmberger\"\n",
    "STUDENTID = \"11915039\"\n",
    "GROUPID = \"3\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf2380c443101b142f56e316de6215fe",
     "grade": false,
     "grade_id": "cell-547da1b1777ed8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 2 (Group)\n",
    "When carrying out a data science project, screening and selecting appropriate data sources for the tasks at hand comes at the beginning. This assignment is about accessing and characterising potential data sources in teams of three. The teams have been randomly assigned.\n",
    "\n",
    "-----\n",
    "## Step 0 (2 points)\n",
    "\n",
    "Find two data sets online (from one or several sources) that would be interesting to combine. The data sets should fulfill the following requirements:\n",
    "\n",
    "* Each data set must have a different file format (either CSV, XML, or JSON), please choose \n",
    " - one CSV file (dataset1) \n",
    " - and one JSON or XML file (dataset2)\n",
    "\n",
    "* Workable data-set sizes: The selected or extracted data sets should have thousands of entries (>= 1000), but not more than (<=) 10000 entries. *If larger, use an excerpt from the original data set. Justify in detail the extraction criteria in the markdown cell below and add the code used for the extraction in the code cell.*\n",
    "* You may start from (but you are not limited to) the resource collections hinted at [in the Unit 2 slides](https://datascience.ai.wu.ac.at/ws21/dataprocessing1/unit2.html#slide-53).\n",
    "\n",
    "* Important: The use of datasets from kaggle.com and other curated collections (as highlighted to you in Unit 2) of datasets with accompanying tutorials on processing and analysis is discouraged. You are required to use primary data sources. See the policy on kaggle.com & friends at this assignment's submission site at MyLearn.\n",
    "\n",
    "* Please adhere to the CoC - It is advised to already do so while working on the assignments.\n",
    "\n",
    "\n",
    "[Data citations](http://blogs.nature.com/scientificdata/2016/07/14/data-citations-at-scientific-data/) must contain the following details:\n",
    "- creator: provider organisation / author(s) of the data set, e.g. \"Zentralanstalt für Meteorologie und Geodynamik (ZAMG)\"\n",
    "- catalogName: Names of the data repository and/or the Open Data portal used, e.g. Open Data Österreich\"\n",
    "- catalogURL: URL of th repository / portal, e.g. \"https://www.data.gv.at/\"\n",
    "- datasetID: (specific to the data repository), e.g. \"https://www.data.gv.at/katalog/dataset/zamg_meteorologischemessdatenderzamg\"\n",
    "- resourceURL: a URL where the CSV, XML or JSON file can be downloaded, e.g. \"https://www.football-data.co.uk/new/JPN.csv\"\n",
    "- pubYear: Dataset publication year, i.e. since when it is published, e.g. \"2012\"\n",
    "- lastAccessed: when have you last accessed the dataset (i.e. datetime of accessing, obtaining a copy of the data set) in ISO Format? e.g. \"2021-03-08T13:55:00\"\n",
    "\n",
    "Store the data citation in a dictionary for each of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d0978891458cae45b1922299869aaf0",
     "grade": true,
     "grade_id": "cell-f5c21aa256cedf80",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dataset1= {\n",
    "    \"creator\" : \"European Centre for Disease Prevention and Control\" ,\n",
    "    \"catalogName\" : \"data.europa.eu\" ,\n",
    "    \"catalogURL\" : \"https://data.europa.eu/\" ,\n",
    "    \"datasetID\" : \"https://data.europa.eu/data/datasets/34ce6bfa-87f3-4f07-82a7-fb5decea1a18?locale=de\" ,\n",
    "    \"resourceURL\" : \"https://opendata.ecdc.europa.eu/covid19/agecasesnational/csv/data.csv\"  ,\n",
    "    \"pubYear\" : \"2021\"  ,\n",
    "    \"lastAccessed\" : \"2021-10-23T17:35:00\"\n",
    "}\n",
    "\n",
    "dataset2= {\n",
    "    \"creator\" : \"European Centre for Disease Prevention and Control\" ,\n",
    "    \"catalogName\" : \"data.europa.eu\" ,\n",
    "    \"catalogURL\" : \"https://data.europa.eu/\" ,\n",
    "    \"datasetID\" : \"https://data.europa.eu/data/datasets/covid-19-testing?locale=en\" ,\n",
    "    \"resourceURL\" : \"https://opendata.ecdc.europa.eu/covid19/testing/json/\"  ,\n",
    "    \"pubYear\" : \"2021\"  ,\n",
    "    \"lastAccessed\" : \"2021-10-23T17:35:00\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c44cbf045edcf21caf73bb6e277a3c40",
     "grade": true,
     "grade_id": "cell-ab8cd6999e00795a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "\n",
    "assert_equal(type(dataset1), dict)\n",
    "assert_equal(type(dataset2), dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff82b0ab70d78c3109817bd2b86a0bcd",
     "grade": false,
     "grade_id": "cell-15330b45683c54aa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following structure for your answer below:\n",
    "\n",
    "**Data set 1**\n",
    "\n",
    "*(Describe the source and the general content of the dataset and why you chose it)*\n",
    "\n",
    "**Data set 2**\n",
    "\n",
    "*(Describe the source and the general content of the dataset and why you chose it)*\n",
    "\n",
    "**Project ideas**\n",
    "\n",
    "*(Describe in your own words, which kind of tasks could be addressed by combining the selected data sets, esp. how the two data sets fit together and what complementary information they contain; what question could be potentially answered by combining data from both datasets; how could the data sets be combined exactly? 250 words max.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12a20659885d8b095062ac68dad8e3a1",
     "grade": true,
     "grade_id": "cell-62681c4240d2770f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Data set 1**\n",
    "\n",
    "It describes the data on the 14-day age-specific notification rate of new COVID-19 cases.\n",
    "The data is structered as a CSV-file with 8 columns. Because of the row count of over 17.000 entries, we will exclude some data to get the row count under 10.000. The data is updated weekly. For this task we focus on excluding the age groups (15yr, 25-49yr, 65-79yr). We chose this data set as we believe that it will give valuable insight into Covid-19 testing. Our excerpt of the data is in the data folder under the name dataset1_excerpt.csv. The code for extraction is written as a comment in Step 1 - File Access.\n",
    "\n",
    "\n",
    "**Data set 2**\n",
    "\n",
    "The second data set contains data on testing for COVID-19 by week and country in the EU. This file contains a dictionary of 12 different key-value pairs for each week (starting with week 15 in 2020; ending with week 42 in 2021 – as of 01.11.2021, the data is updated weekly) on a national and subnational level of EU countries. In each dictionary there are 12 different key-value pairs: country, country_code, year_week, level (subnational or national level data), region, region_name, new_cases, tests_done, population, testing_rate (per 100.000 population), positivity_rate and testing_data_source. We have chosen this data set as we believe that it can be connected well to data set 1 and provides us with additional useful information which can be used for interesting predictions. \n",
    "\n",
    "**Project ideas**\n",
    "\n",
    "The idea for this project is to be able to combine the two data sets and therefore see how many new cases (in percent) belong to a certain age group. This could be useful in order to know which age group is currently the most affected and how this changes over the weeks. The combination of the two data sets could also be used in a machine learning way to predict the future covid cases per week or the number of cases per age group per week. It should be mentioned that the data in both files is being updated regularly as time goes on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1f3c36ffedb8a67dddf0ecfb292ad0c",
     "grade": false,
     "grade_id": "cell-716d568e128ba2a7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## Step 1 - File Access (3 points)\n",
    "\n",
    "Write a python function `accessData` that takes the dataset dictionary created in step 0 as an input and returns an extended of that dictionary back with the following additions:\n",
    "\n",
    "* Write code that accesses the dataset from its `resourceURL`\n",
    " * detects whether it's and XML, CSV or JSON file by\n",
    "     * checking whether the download URL ends with suffix \".xml\", \".json\", \".csv\" or \".tsv\" \n",
    "     * checking whether the \"Content-Type\" HTTP header field contains information about the format, hinting on XML, JSON or CSV.\n",
    " * Detects the file size (convert to KB) of each data set, clearly documenting your actions (e.g. through commented code).\n",
    "\n",
    "The result of the code below should extend your dictionaries `dataset1` and `dataset2` with two keys named \n",
    "* `\"detectedFormat\"` (which has one of the following values: `\"XML\"`, `\"JSON\"`, `\"CSV\"`, or `\"unknown\"`, if nothing could be detected from checking the suffix or HTTP header, or if the information in both was inconsistent)\n",
    "* and `\"filesizeKB\"` which contains the filesize in KB\n",
    "* If the detected format is `\"unknown\"`, the expected filesize is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d99b57da35f065e358ed5c9d74fdc9c3",
     "grade": false,
     "grade_id": "cell-06384a01f3b8fd45",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Code for extraction of the data:\n",
    "\"\"\"\n",
    "import os, csv\n",
    "with open('dataset1_valid.csv', 'r') as inp, open('dataset1_excerpt.csv', 'w') as out:\n",
    "    writer = csv.writer(out)\n",
    "    for row in csv.reader(inp):\n",
    "        if row[3].strip() != \"<15yr\" and row[3].strip() != \"80+yr\" and row[3].strip() != \"65-79yr\":\n",
    "            writer.writerow(row)\n",
    "\"\"\"\n",
    "import requests\n",
    "import urllib.request\n",
    "\n",
    "def accessData(datadict):       \n",
    "    url=datadict[\"resourceURL\"]\n",
    "    req =  urllib.request.Request(url , method=\"HEAD\")\n",
    "    with urllib.request.urlopen(req) as resp:\n",
    "        header = resp.info()\n",
    "        \n",
    "        splittedUrl = datadict[\"resourceURL\"].split(\"/\")\n",
    "        urlEnding = splittedUrl[-1].split(\".\")\n",
    "\n",
    "        if urlEnding[-1].upper() == \"CSV\" or urlEnding[-1].upper() == \"JSON\" or urlEnding[-1].upper() == \"XML\":\n",
    "            datadict[\"detectedFormat\"] = urlEnding[-1].upper()\n",
    "        else:\n",
    "            contentType = header['Content-Type'].split(\"/\")\n",
    "            if contentType[-1].upper() == \"CSV\" or contentType[-1].upper() == \"JSON\" or contentType[-1].upper() == \"XML\":\n",
    "                 datadict[\"detectedFormat\"] = contentType[-1].upper()\n",
    "            else:\n",
    "                datadict[\"detectedFormat\"] = \"unknown\"\n",
    "        if datadict[\"detectedFormat\"] == \"unknown\":\n",
    "            datadict[\"filesizeKB\"] = 0\n",
    "        else:\n",
    "            datadict[\"filesizeKB\"] = (int(header['Content-Length']) / 1000)\n",
    "    \n",
    "    return datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0011de4928cc4e3c939ef2106dac4569",
     "grade": true,
     "grade_id": "cell-f3baf6d625b220d4",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "dataset1= accessData(dataset1)\n",
    "dataset2= accessData(dataset2)\n",
    "assert_in(dataset1[\"detectedFormat\"], [\"XML\", \"JSON\", \"CSV\", \"unknown\"])\n",
    "assert_in(dataset2[\"detectedFormat\"], [\"XML\", \"JSON\", \"CSV\", \"unknown\"])\n",
    "assert_true(isinstance(dataset1[\"filesizeKB\"], (int, float)))\n",
    "assert_true(isinstance(dataset2[\"filesizeKB\"], (int, float)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9c38140f37c287d4a13bee434f0b4b6",
     "grade": false,
     "grade_id": "cell-773b0383bfb8ea05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Please explain your findings, using the following structure for your answer below (in \"other remarks\" you can explain for instance why you think your code did not detect the correct format, if needed)\n",
    "\n",
    "**Data set 1**\n",
    "\n",
    "*(format, size, other remarks)*\n",
    "\n",
    "\n",
    "**Data set 2**\n",
    "\n",
    "*(format, size, other remarks)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8da38a366613adecbf1162bb8ac19267",
     "grade": true,
     "grade_id": "cell-55569ce66deb1db2",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Data set 1**\n",
    "Data set 1 is a CSV file. The size of the file is 1157.397KB (as of 01.11.2021). The size will differ after they update the dataset. Our code didn't have any issues detecting the format. When we only use the excerpt of the data set 1, our file size is 581.917KB (as of 01.11.2021).\n",
    "\n",
    "\n",
    "**Data set 2**\n",
    "Data set 2 is a JSON file. The size of the file is 3438.952KB.\n",
    "For our json file, our code couldn't detect the .json file at first, because our link to the JSON file was MIME type, meaning it wasn't written as .json, but /json. We managed to fix it by splitting the url from the dataset2 dictionary from above based on \"/\", and then splitting it again based on \".\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4bd16698859e953f56b0b627be3084f",
     "grade": false,
     "grade_id": "cell-41258b830f38673b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## Step 2  (5 points) - Format Validation\n",
    "\n",
    "Establish that the two data files obtained are well-formed according to the detected data format (CSV, JSON, or XML). That is, the syntax used is valid according to accepted syntax definitions. Are there any violations of well-formedness?\n",
    "\n",
    "\n",
    "Proceed as follows (for each data file, in turn): according to the \"suspected\" data format from Step 1:\n",
    "\n",
    "  1. Use an _online validator_ for CSV, XML, and JSON, respectively, to confirm whether the files you downloaded in Step 1 are well-formed for the respective file format, document your findings and modify the file as described: \n",
    "\n",
    "   a. **Case 1**: no well-formedness errors were detected: \n",
    "    * Generally describe at least 3 well-formedness checks that your data sets, depending on its \"suspected\" format (against the background knowledge of Unit 2) should fulfill;\n",
    "    * Store a local copy of the file called `dataset1_valid` (or, respectively, `dataset2_valid`) in the `data/` subfolder\n",
    "    * Create another local copy of your data file called `dataset1_invalid` (or, respectively, `dataset2_invalid`) and introduce a selected well-formedness violation (one occurrence) therein;\n",
    "    * document that the online validator you used finds the error you introduced\n",
    "\n",
    "   b. **Case 2**: well-formedness errors occurred:\n",
    "    * Document the occurrences by printing out the error message and describe the types of well-formedness violation that were reported to you.\n",
    "    * Store a local copy called `dataset1_invalid` (or, respectively, `dataset2_invalid`) in the `data/ subfolder`\n",
    "    * Create another local copy called `dataset1_valid` (or, respectively, `dataset2_valid`), of your data file that fixes the well-formedness violations therein manually.  \n",
    "    \n",
    "\n",
    "  2. Write a Python function `parseFile(datadict, format)` that that accesses the dataset from its `resourceURL`. The dataset should then be checked accordingly the given parser for the parameter `format` to check the following:\n",
    "     * CSV: Returns `True`, if a consistent delimiter out of `\",\",\";\",\"\\t\"` can be detected, such that each row has the same (> 1) number of elements, otherwise False\n",
    "     * JSON: Returns `True` if the file can be parsed with the `json` package, catching any parsing exceptions.\n",
    "     * XML: Returns `True` if the file can be parsed with the `xmltodict` package, catching any parsing exceptions.\n",
    "     * Returns `False` if any other format is supplied by the parameter.\n",
    "     \n",
    "In order to handle parsing exceptions and errors from the used packages, you can use [catching exceptions](https://docs.python.org/3/tutorial/errors.html), such that the program does not simply fail to check whether the file is parseable as the format specified in `format`     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01920d8480c77e3b166703b82061e806",
     "grade": false,
     "grade_id": "cell-3c0ae5fb02e6f352",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following structure for your answer in the cell below to document **Step 2.1**:\n",
    "\n",
    "***Data set 1***\n",
    "\n",
    "*(validator used, validation results, describe the modification to fix the file or to create an invalid version of it)*\n",
    "\n",
    "***Data set 2***\n",
    "\n",
    "*(validator used, validation results, describe the modification to fix the file or to create an invalid version of it)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "93cf6e7530dfb8ecc91c0f6b7496d1d6",
     "grade": true,
     "grade_id": "cell-bde7e30e877919a8",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "***Data set 1***\n",
    "Case 1: Some well-formedness errors that could occur in a CSV file are: there being different number of fields in different lines in the file, each record not being on a seperate line or there being more than one field within each record or the header.\n",
    "To validate the CSV file (dataset1), we used csvlint.io. There were no well-formedness errors detected. To modify the file (to create an invalid version of it), we just added an extra field in the third line, which caused the validator to detect a structural problem (Row 3 contains a different number of columns to the first row in the CSV file).\n",
    "\n",
    "\n",
    "***Data set 2***\n",
    "Case 1: Some well-formedness errors that could occur in JSON files are: a JSON object not being enclosed in {}, keys and values not being separated by : but something else or key value pairs not being separated by a comma (,).\n",
    "To validate the JSON file (dataset1), we used jsonformatter.curiousconcept.com. There were no well-formedness errors detected. To modify the file (to create an invalid version of it), we removed a comma between two key-value pairs in one dictionary. The error we got is: Expecting comma or }, not string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "abb86551a6d055aefb4c3cf1948b7f3d",
     "grade": false,
     "grade_id": "cell-b485c10ffce5955c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import json\n",
    "import xmltodict\n",
    "import urllib.request\n",
    "def parseFile(datadict, format):\n",
    "    url=datadict[\"resourceURL\"]\n",
    "    delimiters = [\",\", \";\", \"\\t\"]\n",
    "    l=list()\n",
    "    if format == \"CSV\":\n",
    "        def checkDelimitersWithColumnNumber(columnNumber):\n",
    "            i = 0\n",
    "            for elem in lines:\n",
    "                i += 1\n",
    "                if (len(elem.split(\",\")) != columnNumber and len(elem.split(\";\")) != columnNumber and len(elem.split(\"\\t\")) != columnNumber):\n",
    "                    return False\n",
    "\n",
    "                if i>=1:\n",
    "                    break\n",
    "            \n",
    "            return True;\n",
    "        \n",
    "        l1 = list()\n",
    "        response = urllib.request.urlopen(url)\n",
    "        lines = [l.decode('utf-8') for l in response.readlines()]\n",
    "        cr = csv.reader(lines)\n",
    "        for row in cr:            \n",
    "            l1.append(len(row))\n",
    "        if len(set(l1)) == 1 and l1[0] > 1:\n",
    "            return checkDelimitersWithColumnNumber(l1[0])\n",
    "        else:\n",
    "            return False\n",
    "    elif format == \"XML\":\n",
    "        response = requests.get(url)\n",
    "        try:\n",
    "            data = xmltodict.parse(response.content)\n",
    "            return True\n",
    "        except xmltodict.expat.ExpatError:\n",
    "            return False\n",
    "    elif format == \"JSON\":\n",
    "        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        try:\n",
    "            data = json.loads(response.read())\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08c2e3a5b8591fbb56e57e886be4952d",
     "grade": true,
     "grade_id": "cell-ec533b2ff184c9a5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "assert_equal([parseFile(dataset1, \"XML\"),\n",
    "    parseFile(dataset1, \"JSON\"),\n",
    "    parseFile(dataset1, \"CSV\"),\n",
    "    parseFile(dataset2, \"XML\"),\n",
    "    parseFile(dataset2, \"JSON\"),\n",
    "    parseFile(dataset2, \"CSV\")].count(True), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "270cde094468de1b8cf2eafcbe560381",
     "grade": false,
     "grade_id": "cell-c52ec8e4f4f35f88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## Step 3 - Content analysis (5 points)\n",
    "\n",
    "Similar to the Python function `parseFile(datadict,format)` above, now create a new Python function `describeFile(datadict)` that analyses the given file according to the respective format detected in Step 1 and returns a dictionary containing the following information:\n",
    "\n",
    "* for CSV files: number of columns, number of rows, column number (from 0 to n) of the column which contains the longest text. That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfColumns:\"  ...,\n",
    "       \"numberOfRows\":  ... ,\n",
    "       \"longestColumn\" : ... }\n",
    "    ```\n",
    "\n",
    "* for JSON files: number of different attribute names, nesting depth, length of the longest list appearing in an attribute value. That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfAttributes:\" ... ,\n",
    "      \"nestingDepth\":  ... ,\n",
    "      \"longestListLength\" : ... }\n",
    "     ```\n",
    "\n",
    "  Here the `longestListLength` should be set to 0 if no list appears. [Nesting depth](https://www.tutorialspoint.com/find-depth-of-a-dictionary-in-python) is defined as follows: \n",
    "   * a flat JSON object with only atomic attribute values has depth 1. \n",
    "   * a JSON attribute with another object as value (or another oject as member of a list value!) increases the depth by 1\n",
    "   * and so on.\n",
    "\n",
    "\n",
    "* for XML files: number of different element and attribute a names (i.e. the sum of both), nesting depth, maximum number of child nodes in any element (including the root element). That is, the resulting dictionary should have the following form:\n",
    "\n",
    "    ```\n",
    "    { \"numberOfElementsAttributes:\" ... ,\n",
    "      \"nestingDepth\":  ... ,\n",
    "      \"maxChildren\" : ... }\n",
    "     ```\n",
    "\n",
    "  Here the `maxChildren` should be set to 0 if only a root element appears. Nesting depth is defined as the nesting depth of elements.\n",
    "  \n",
    "For files that do cannot be parsed with respective given format, the function should simply return an empty dictionary (`{}`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bcc200289e65ce43d09a5d9fc26ce86",
     "grade": false,
     "grade_id": "cell-13db3349e6a16395",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def describeFile(datadict):\n",
    "    if parseFile(datadict, datadict[\"detectedFormat\"]) == False:\n",
    "        return {}\n",
    "    \n",
    "    url=datadict[\"resourceURL\"]\n",
    "    \n",
    "    # describe CSV file\n",
    "    if datadict[\"detectedFormat\"] == \"CSV\":    \n",
    "        response = urllib.request.urlopen(url)\n",
    "        lines = [l.decode('utf-8') for l in response.readlines()]\n",
    "        cr = csv.reader(lines)\n",
    "        \n",
    "        maxColumn = {\"columnNumber\" : 0, \"length\" : 0}\n",
    "        rowCount = 0\n",
    "        for row in cr:\n",
    "            rowCount += 1\n",
    "            if rowCount > 1:\n",
    "                columnCount = len(row) \n",
    "                \n",
    "                # determine longest column\n",
    "                i = 0\n",
    "                while i < columnCount:\n",
    "                    if len(row[i]) > maxColumn[\"length\"]:\n",
    "                        maxColumn[\"length\"] = len(row[i])\n",
    "                        maxColumn[\"columnNumber\"] = i\n",
    "                    i += 1\n",
    "        return {\"numberOfColumns\": columnCount, \"numberOfRows\": rowCount, \"longestColumn\" : maxColumn[\"columnNumber\"]+1}\n",
    "    \n",
    "    # describe JSON file\n",
    "    elif datadict[\"detectedFormat\"] == \"JSON\":\n",
    "        # recursive function to get nestingDepth and longestListLength\n",
    "        def getElementInfos(element):\n",
    "            maxDepth = 0\n",
    "            maxList = 0\n",
    "            \n",
    "            for attribute in element:\n",
    "                tempDepth = 1\n",
    "                if isinstance(attribute, dict):\n",
    "                    # get nestingDepth\n",
    "                    tempDepth += getDepth(attribute)\n",
    "                    \n",
    "                    if tempDepth > maxDepth:\n",
    "                        maxDepth = tempDepth\n",
    "                    \n",
    "                    # get longestListLength\n",
    "                    len_lists = list()\n",
    "                    for key in attribute:\n",
    "                        if type(attribute.get(key)) == list:\n",
    "                            len_lists.append(len(attribute.get(key)))\n",
    "                        if len(len_lists) > 0:\n",
    "                            maxList = max(len_lists)\n",
    "                        else:\n",
    "                            maxList = 0\n",
    "                else:\n",
    "                    if maxDepth < 1:\n",
    "                        maxDepth = 1\n",
    "                \n",
    "            return {\"maxDepth\": maxDepth, \"maxList\": maxList};\n",
    "        \n",
    "        response = urllib.request.urlopen(url)\n",
    "        data = json.loads(response.read())\n",
    "        \n",
    "        maxNestingDepth = 0\n",
    "        maxListLength = 0\n",
    "        \n",
    "        # interate over every JSON element and get it's depth and longest list length\n",
    "        # then define it as maximum value, if it is\n",
    "        for element in data:\n",
    "            elementInfos = getElementInfos(element)\n",
    "            if elementInfos[\"maxDepth\"] > maxNestingDepth:\n",
    "                maxNestingDepth = elementInfos[\"maxDepth\"]\n",
    "            \n",
    "            if elementInfos[\"maxList\"] > maxListLength:\n",
    "                maxListLength = elementInfos[\"maxList\"]\n",
    "        \n",
    "        return {\"numberOfAttributes\": len(data[0].keys()), \"nestingDepth\": maxNestingDepth, \"longestListLength\" : maxListLength}\n",
    "    \n",
    "    # describe XML file\n",
    "    elif datadict[\"detectedFormat\"] == \"XML\":        \n",
    "        # read the XML file and convert it to JSON\n",
    "        with urllib.request.urlopen(url) as f:\n",
    "            x=(xmltodict.parse(f.read()))\n",
    "        res = json.dumps(x)\n",
    "        res_dict = res.split(\"[\")[1].split(\"]\")[0]\n",
    "        data = list(eval(res_dict))\n",
    "        \n",
    "        def getElementInfos(element):\n",
    "            maxDepth = 0\n",
    "            maxChild = 0\n",
    "            \n",
    "            for attribute in element:\n",
    "                tempDepth = 1\n",
    "                \n",
    "                if isinstance(attribute, dict):\n",
    "                    # get nestingDepth\n",
    "                    tempDepth += getDepth(attribute)\n",
    "                    \n",
    "                    if tempDepth > maxDepth:\n",
    "                        maxDepth = tempDepth\n",
    "                    \n",
    "                    len_lists = list()\n",
    "                    for key in attribute:\n",
    "                        if type(attribute.get(key)) == list or type(attribute.get(key)) == dict:\n",
    "                            len_lists.append(len(attribute.get(key)))\n",
    "                        if len(len_lists) > 0:\n",
    "                            maxChild = max(len_lists)\n",
    "                        else:\n",
    "                            maxChild = 0\n",
    "                else:\n",
    "                    if maxDepth < 1:\n",
    "                        maxDepth = 1\n",
    "                \n",
    "            return {\"maxDepth\": maxDepth, \"maxChild\": maxChild};\n",
    "       \n",
    "        maxNestingDepth = 0\n",
    "        maxChildren = 0\n",
    "        \n",
    "        # interate over every JSON element and get it's nesting depth and maximum children\n",
    "        # then define it as maximum value, if it is\n",
    "        for element in data:\n",
    "            elementInfos = getElementInfos(element)\n",
    "            if elementInfos[\"maxDepth\"] > maxNestingDepth:\n",
    "                maxNestingDepth = elementInfos[\"maxDepth\"]\n",
    "            if elementInfos[\"maxChild\"] > maxChildren:\n",
    "                maxChildren = elementInfos[\"maxChild\"]\n",
    "        return {\"numberOfElementsAttributes:\": len(data[0].keys()), \"nestingDepth\": maxNestingDepth, \"maxChildren\" : maxChildren}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b51921191f3a2046dfc559554d674827",
     "grade": true,
     "grade_id": "cell-b60c630b460e5225",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_in, assert_true\n",
    "assert_equal(len(describeFile(dataset1)), 3)\n",
    "assert_equal(len(describeFile(dataset2)), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73eeeca5d51dd87582ea05e6eeb8981e",
     "grade": false,
     "grade_id": "cell-ddc7be12117aa616",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Use the following structure for your answer below:\n",
    "\n",
    "**Data set 1**\n",
    "\n",
    "*(number and types of items etc.)*\n",
    "\n",
    "\n",
    "**Data set 2**\n",
    "\n",
    "*(number and types of items etc.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50d2a3bf295971bbc75b4b6517499caf",
     "grade": true,
     "grade_id": "cell-bebda395c911c793",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Data set 1**\n",
    "\n",
    "At the time of writing (01.11.2021), data set 1 has 8 columns, 17101 rows and the longest length of a column is 7.\n",
    "\n",
    "**Data set 2**\n",
    "\n",
    "At the time of writing (01.11.2021), data set 2 has 12 attributes, a nesting depth of 1 and the longest list length is 0, meaning there are no lists (nested attributes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
