{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2120badbc98de39b433d8e3b7abaacaf",
     "grade": false,
     "grade_id": "cell-9e78272cc4af091a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "*General hints:* <br>\n",
    "* You may use another notebook to test different approaches and ideas. When complete and mature, turn your code snippets into the requested functions in this notebook for submission. \n",
    "* Make sure the function implementations are generic and can be applied to any dataset (not just the one provided).\n",
    "* Add explanatory code comments in the code cells. Make sure that these comments improve our understanding of your implementation decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca384ab2b432240f2e75945bb1878249",
     "grade": false,
     "grade_id": "cell-9d52cb434e7f0910",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "* Create a variable holding your student id, as shown below. \n",
    "* Simply replace the example (`01234567`) with your actual student id having a total of 8 digits. \n",
    "* Maintain the variable as a string, do NOT change its type in this notebook!\n",
    "* *Note: If your student id has 7 digits, add a leading 0. The final student id MUST have 8 digits!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = '11915039'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3489966489e6d6c742161745f2e61bc3",
     "grade": false,
     "grade_id": "cell-420b4c449379ffe5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 0. Import\n",
    "\n",
    "Implement a function `tidy` which imports the data set assigned and provided to you as a CSV file into a `pandas` dataframe. Access the data set and establish whether your data set is tidy. If not, clean the data set before continuing with Step 1. Mind all rules of tidying data sets in this step. Make sure you comply to the following statements:\n",
    "* If there is an index column (row numbers) in your tidied dataset, keep it.\n",
    "* The following columns, once identified, correspond to variables 1:1 (no need for transformations):\n",
    "  * `full_name`\n",
    "  * `automotive`\n",
    "  * `color`\n",
    "  * `job`\n",
    "  * `address`\n",
    "  * `coordinates`\n",
    "* The tidied dataset should have a total of 8 columns (not including the index), the first column should be `full_name`.\n",
    "* Mind the intended content of each attribute (e.g. full_name should contain the full name of a person, no need to change that)\n",
    "* If tidy or done, have the function `tidy` return the ready data set as a dataframe.\n",
    "\n",
    "Note that `tidy` must take a single parameter that holds the basename of the CSV file (i.e., the name without file extension). Do NOT change the name of the file, do not overwrite the original data file, and make sure you submit your final ZIP following the [Code of Conduct](https://datascience.ai.wu.ac.at/ws21/dataprocessing1/code_of_conduct.html) requirements. Especially, make sure you put your data file in a folder called `data/` when submitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e96fea9cc9f84652b592c3659339a48b",
     "grade": false,
     "grade_id": "cell-81e21dccd785e63d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def tidy(x):\n",
    "    csv = \"./data/\" + x + \".csv\"\n",
    "    df = pandas.read_csv(csv)\n",
    "\n",
    "    if len(df.columns) > len(df.index):\n",
    "        # transpose the dataframe\n",
    "        df = df.T\n",
    "\n",
    "    if pandas.Series([\"full_name\",\"automotive\",\"color\",\"job\",\"address\",\"coordinates\"]).isin(df.iloc[0]).all():\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df.drop(df.index[0])\n",
    "        \n",
    "    # split last column\n",
    "    lastColumn = str(df.columns[-1]).split('/')\n",
    "    colDate = lastColumn[0]\n",
    "    colCompany = lastColumn[1]\n",
    "    df.rename(columns={df.columns[-1]: colDate}, inplace=True)\n",
    "    df[colCompany] = df.columns[-1]\n",
    "\n",
    "    from dateutil.parser import parse\n",
    "    import numpy as np\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            date = parse(row[colDate][0:19], fuzzy_with_tokens=True)\n",
    "        except ValueError:\n",
    "            # get the first alphabetic letter from the value, to identify the beginning of the company name\n",
    "            indexAlpha = 0\n",
    "            for s in row[colDate]:\n",
    "                if s.isalpha():\n",
    "                    break\n",
    "                indexAlpha += 1\n",
    "                \n",
    "            row[colCompany] = row[colDate][indexAlpha:]\n",
    "            row[colDate] = np.nan\n",
    "            continue\n",
    "\n",
    "        row[colCompany] = row[colDate][19:]\n",
    "        row[colDate] = str(date[0])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21071c01488e30d64223a1d4cdc68565",
     "grade": true,
     "grade_id": "cell-9a75a2763c7bbe5d",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "import pandas\n",
    "assert_equal(type(tidy(mn)), pandas.core.frame.DataFrame)\n",
    "assert_equal(len((tidy(mn)).columns), 8)\n",
    "assert_equal(list((tidy(mn)).columns)[0], \"full_name\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd3044074dc0c4dd2d7206a6e4f295d3",
     "grade": false,
     "grade_id": "cell-72c2573051ab8535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-------\n",
    "## 1. Missing values\n",
    "\n",
    "### 1.1 Code part\n",
    "Implement a function called `missing_values` which takes as an input a dataframe and check if there are any missing values in the dataset. Record the row ids of the observations containing missing values as a list of numbers and make sure that the function returns the recorded list in the end. If there are no missing values, `missing_values` should return an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c924e80de70e8169cc282686502fb094",
     "grade": false,
     "grade_id": "cell-6d4fb7f2e44e7cfb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def missing_values(x):\n",
    "    df = x[x.isna().any(axis=1)]\n",
    "    \n",
    "    return df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a97d5951438fefbe421ce910b4a576a7",
     "grade": true,
     "grade_id": "cell-4c692eab5b638fc7",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(type(missing_values(tidy(mn))), list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52fac7bcbfdfcaea74bdcda572dac51b",
     "grade": false,
     "grade_id": "cell-82f316abfa9423e7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1.2. Analytical part\n",
    "\n",
    "* Does the dataset contain missing values?\n",
    "* If no, explain how you proved that this is actually the case.\n",
    "* If yes, describe the discovered missing values. What could be an explanation for their missingness?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "229a22e8b0e33f07eb7e3e7a6a652bf2",
     "grade": true,
     "grade_id": "cell-3ad38c6f2d1998f6",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Yes the dataset contains missing values. The missing values occur randomly in every column of the dataset, so I would describe it as MCAR data, because occurrence that a data is missing is unrelated to the value itself or any other variable in the data set.\n",
    "An explanation for the missing values could be that the data gathering process itself wasn't that sophisticated. Because in the original data version, the two columns had been put together, therefore I would argue, that then the likeability is also high, that there a missing values inside the data observations.\n",
    "\n",
    "Another point I want to mention is, that it is not fully possible to get the company name with a 100% accuracy, because there is the possibility that the term \"None\" is actually part of the company name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d56c3e9c89d20a3ef1cf3dd09ada7a81",
     "grade": false,
     "grade_id": "cell-87126fc406d941ad",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "------\n",
    "## 2. Handling missing values\n",
    "### 2.1. Code part\n",
    "Apply a (simple) function called *handling_missing_values* for handling missing values using an adequate single-imputation technique  of your choice per type of missing values. Make use of the techniques learned in Unit 4. The function should take as an input a dataframe and return the updated dataframe. Mind the following:\n",
    "- The objective is to apply single imputation on these synthetic data. Do not make up a background story (at this point)!\n",
    "- Do NOT simply drop the missing values. This is not an option.\n",
    "- The imputation technique must be adequate for a given variabel type (quantitative, qualitative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0226044c2b66de952ca20bc64edae12b",
     "grade": false,
     "grade_id": "cell-5ba2dc8b5cbe1f8b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def handling_missing_values(x):\n",
    "    # average datetime\n",
    "    dates = []\n",
    "    from dateutil.parser import parse\n",
    "    for index, row in x.iterrows():\n",
    "        try:\n",
    "            date = parse(str(row['date_time']), fuzzy_with_tokens=True)\n",
    "            dates.append(date[0])\n",
    "        except ValueError:\n",
    "            continue\n",
    " \n",
    "    import numpy as np\n",
    "    mean = (np.array(dates, dtype='datetime64[s]')\n",
    "            .view('i8')\n",
    "            .mean()\n",
    "            .astype('datetime64[s]'))\n",
    "    \n",
    "    meanDate = parse(str(mean), fuzzy_with_tokens=True)[0]\n",
    "    missing = missing_values(x)\n",
    "    \n",
    "    dfCommonValues = x\n",
    "    \n",
    "    for row in missing:\n",
    "        for columnIndex, value in x.iloc[int(row)].items():\n",
    "            if str(value) == \"nan\":\n",
    "                if columnIndex == \"date_time\":\n",
    "                    # fill with mean from dates\n",
    "                    x.iloc[int(row)]['date_time'] = meanDate\n",
    "                else:\n",
    "                    # fill with most frequent value from the qualitative column\n",
    "                    x.iloc[int(row)][columnIndex] = dfCommonValues[columnIndex].value_counts().idxmax()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f4a5257fd57591d9ae8374dc832e2c9a",
     "grade": true,
     "grade_id": "cell-0edce052e98e2e95",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(len(missing_values(handling_missing_values(tidy(mn)))), 0)\n",
    "assert_equal(handling_missing_values(tidy(mn)).shape, tidy(mn).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "447f361ce9e1ad6d11aa704458392787",
     "grade": false,
     "grade_id": "cell-c697641b9d5a1c3a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.2. Analytical part\n",
    "Discuss the implications. Answer the following:\n",
    "- How would you qualify the data-generating processes leading to different types of missing values, provided that the data was not synthetic?\n",
    "- What are the benefits and disadvantages of the chosen single-imputation technique?\n",
    "- How would you apply a multiple-imputation technique to one type of missing values, if applicable at all?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34f9ca681c722ebe151d6fac42d71b25",
     "grade": true,
     "grade_id": "cell-5c05456587f2ff17",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- Qualify the data-generating process:\n",
    "    - I had missing values in every column. One type of the missing values was in the date_time column, a quantitative value, therefore I calculated the mean and inserted the mean in the empty cells. But in my opinion it's not the best way to handle this, because the mean doesn't fit for DateTimes that good, because you can't add it up in a sophisticated way.\n",
    "\n",
    "    - The second type of missing values was in every other columns. The type was a qualitative value, therefore i calculated the most common value for each column, and filled the missing cells in the dataset with it.\n",
    "The column \"coordinates\" includes two decimal values, from which the mean could be calculated, but according to what I heard in the lecture, it's not neccesaryly needed to convert the coordinates to numeric values and then make special calucaltions with it.\n",
    "\n",
    "- It has positive aspects because you can execute complete case analysis of the sample, but in contrast the missing values which are filled in are mostly identically (especially with qualitative data), which has effects on the variance of the data.\n",
    "\n",
    "- Multiple-Imputation technique:\n",
    "    - I would probably go with an Hot-Deck approach, to generate values for the qualitative missing data, for instance the coordinates. I would take qualitative variable address and maybe also the qualitative variable automotive in consideration to form a contingency table. In this way I would approach to get a useful mean to fill the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dfaf0570a8a70924b35ef279df754c19",
     "grade": false,
     "grade_id": "cell-573d56d6699b84eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 3. Duplicate entries\n",
    "Implement a function called `duplicates` that takes as an input a (tidy) dataframe `x`. Assume that `duplicates` receives a dataframe as returned from your Step 0 implementation of `tidy`. It then checks whether there are any duplicates in the dataset. Record the row ids of the observations being duplicates and have `duplicates` returns the list in the end. An empty list indicates the absence of duplicated observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d3199cb48685917455f085c8d366844",
     "grade": false,
     "grade_id": "cell-7954cffea933a812",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def duplicates(x):\n",
    "    df = x[x.duplicated()]\n",
    "    \n",
    "    return df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "81f1ddaa2595165edcbe3c3848bf1880",
     "grade": true,
     "grade_id": "cell-583bc8ab4ba38aa6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(type(duplicates(tidy(mn))), list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8078773ced981185cbbd33775ad95033",
     "grade": false,
     "grade_id": "cell-b04e3f6689a78a44",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "-----\n",
    "## 4. Handling duplicate entries\n",
    "### 4.1. Code part\n",
    "Implement a function called `handling_duplicate_entries` for handling duplicate entries. Again, the function is assumed to receive a tidied data set as obtained from Step 0. It deduplicates the tidy data set. The function then returns the dataframe without duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "612e2d5322a7c96c67c1405ff80ebb36",
     "grade": false,
     "grade_id": "cell-f593e79eae8f4227",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def handling_duplicate_entries(x):\n",
    "    return x.drop(duplicates(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "10bdb20c5d61c818cc5ff269c76ccb62",
     "grade": true,
     "grade_id": "cell-231217b55b6ef843",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal\n",
    "assert_equal(len(duplicates(handling_duplicate_entries(tidy(mn)))), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "871ab23c87eefc9af50d1a3ae12392d8",
     "grade": false,
     "grade_id": "cell-8c4530b128b5c186",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 4.2. Analytical part\n",
    "Discuss the implications. \n",
    "\n",
    "- What are the benefits and disadvantages of the chosen duplicate-handling technique?\n",
    "- What are alternative definitions of (intra-source) duplicates for the given dataset?\n",
    "\n",
    "Write your answer in the markdown cell bellow. Do NOT delete or replace the answer cell with another one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e93305db24e2fc3800f252b797c2ad7",
     "grade": true,
     "grade_id": "cell-254db63097c3ed40",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "- I choose to find out full duplicates, which means that I searched for identically observations in the dataset\n",
    "    - the benefit is, that the observations in the dataset will be more unique\n",
    "    \n",
    "    - a possible negative aspect of the observation could be, that every single value of the observation has to be compared with the value of another observation, which takes a lot more time, than for example if I would only look for duplicates in certain columns\n",
    "    \n",
    "- Alternative definitions could be\n",
    "    - regarding the \"full_names\" column, names that are written differntly concerning upper and lower case letters\n",
    "    - regarding the \"automotive\" column, when the \"-\" between the numbers and values is occuring or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
